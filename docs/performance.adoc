= Performance tests

Tackler's performance is tested with artificial txn data
which mimics as well as possible real transactional data.


Tests are done with balance report and with setting `accounts.strict=true` 
(e.g. for each transaction it is checked that all accounts are listed in
Chart of Accounts).

Each transaction is located on own file, and shard of files is based on txn dates
(e.g. `perf-1E6/YYYY/MM/DD/YYYYMMDDTHHMMSS-idx.txn`, `idx` is index of txn).

Typical test sets (small and big) are:

 * 1E3 (1000) transactions
 * 1E6 (1 000 000) transactions

Other sets are 1E4 and 1E5.


== Chart of Accounts for perf tests

Chart of Accounts is 380 entries and it is based on txn's dates:

For "assets" (`a:ay<year>:am<month>`):

 ...
 "a:ay2016:am10",
 "a:ay2016:am11",
 "a:ay2016:am12",
 ...


For "expenses" (`e:ey<year>:em<month>:ed<day>`):

 ...
 "e:ey2016:em01:ed01",
 "e:ey2016:em01:ed02",
 "e:ey2016:em01:ed03",
 ...


== Performance test hardware

Reference performance tests are done with normal laptop:
 
 * CPU: Intel(R) Core(TM) i5-2540M CPU @ 2.60GHz
 * Harddrive: Intel(R) SSD 535
 * Memory: 16GB

CPU has two cores and four threads. All txns data and program data
can be cached in memory by OS with this test setup.


== Performance

Tackler has good performance, and it is actually faster than native application
ledger-cli with similar memory footprint. Main reason is that ledger-cli
utilizes only one core, when in other hand Tackler utilizes all available cores
and threads.


== With shard txn data

Tacklers processing time with shard txn data is following:

 * 1E3: 3.1 sec
 * 1E6: 31 sec

With 1E6 data set used memory peaks around 2.2GB.

Full results are located under link:../perf/results[perf results].


=== Single file or single input String

With single file Tackler has basically same processing time than with
shard txn data.

However, with 1E6 txns in single input file or in one single string (about 73MB),
Tackler has non-optimal memory usage behaviour, and it's memory usage peaks around 6-7GB.

To work around this, do not put one million (1E6) transactions in single input.
Instead this should be shard data set this data set of e.g. ten files (1E5 txns on each).

Shard data set could be even structured so that there one or only few txn per file.
This is basic mode for performance test, and this setup has basically same processing
time than putting them in one big file (with fast SSD and good disk cache).


==== Single file performance

....
time java -Xms8192M -jar tackler-cli-0.4.0.jar --cfg perf.conf --input.file single/perf-1E6.txn --output out/foo
Txns size: 1000000

Total processing time: 28351, input: 1, parse: 24766, reporting: 1308

real	0m28.962s
user	0m49.389s
sys	0m2.644s
.....

This is not part of routine perf testing.
